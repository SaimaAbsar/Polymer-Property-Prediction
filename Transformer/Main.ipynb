{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c1bf317b-2daf-4de4-ab09-78ee19bb575e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This is the main script that reads the tokenized SMILES data, trains a self-supervised masked language model, and uses a fully-connected linear layer to train and predict the 5 target polymer properties."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1bb99552-51e7-4e93-9351-9624d55d36ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###1. Use the Tokenization notebook to create tokens for train and test data using TransPolymer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d010024-fb32-4282-8bb3-9fac6807e7e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###2. Train the self-supervised model with masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faf9e9ea-56a2-44bf-a9ff-b9e4c76cda0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "TP_DIR=\"\" #main project directory\n",
    "VOCAB=\"$TP_DIR/tokenizer/vocab.json\"\n",
    "MERGES=\"$TP_DIR/tokenizer/merges.txt\"\n",
    "TOKENS_TRAIN=\"/data/train.tokenized.pt\" # not added to the repo\n",
    "OUT_SSL=\"/Transformer/saved_models/ssl_mlm_ckpt\"\n",
    "\n",
    "export PYTHONPATH=\"$TP_DIR:$PYTHONPATH\"\n",
    "export TOKENS_TRAIN OUT_SSL\n",
    "\n",
    "python self_supervised_mlm.py \\\n",
    " --tp_dir \"$TP_DIR\" \\\n",
    " --vocab \"$VOCAB\" \\\n",
    " --merges \"$MERGES\" \\\n",
    " --tokens_pt \"$TOKENS_TRAIN\" \\\n",
    " --out_dir \"$OUT_SSL\" \\\n",
    " --max_len 128 --hidden 256 --layers 6 --heads 8 --intermediate 1024 --batch_size 32 --epochs 5 --lr 5e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6af1333a-84cd-4cbe-b3da-6f9d0b6db29a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###3. Train the regressor to predict polymer properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db0a5865-689e-4515-ab9f-1ee086e4b32b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "TRAIN_DATA=\"/data/train.csv\"  # not added to the repo\n",
    "TOKENS_TRAIN=\"/data/train.tokenized.pt\" # not added to the repo\n",
    "SSL_DIR=\"Transformer/saved_models/ssl_mlm_ckpt\"\n",
    "OUT_REGRESSOR=\"/Transformer/saved_models/regressor_ckpt\"\n",
    "\n",
    "export TRAIN_DATA TOKENS_TRAIN SSL_DIR OUT_REGRESSOR\n",
    "\n",
    "python regressor.py \\\n",
    "  --input_data \"$TRAIN_DATA\" \\\n",
    "  --tokens_pt \"$TOKENS_TRAIN\" \\\n",
    "  --ssl_dir \"$SSL_DIR\" \\\n",
    "  --out_dir \"$OUT_REGRESSOR\" \\\n",
    "  --epochs 20 --batch_size 32 --lr 2e-4 --freeze_encoder_epochs 3 --pool cls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25e49e07-bf1f-4b7c-99bb-f7b7f643923f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###4. Prediction on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20cf38b2-c940-4a76-afc6-2a8364d3156d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "TEST_DATA=\"/data/test.csv\" # not added to the repo\n",
    "TOKENS_TEST=\"/data/test.tokenized.pt\" # not added to the repo\n",
    "REG_DIR=\"/Transformer/saved_models/regressor_ckpt\" # not added to the repo\n",
    "OUT=\"/Transformer/results/predictions_test.csv\"\n",
    "\n",
    "export TOKENS_TEST REG_DIR TEST_DATA OUT\n",
    "\n",
    "python predict_test.py \\\n",
    "  --tokens_pt \"$TOKENS_TEST\" \\\n",
    "  --regressor_dir \"$REG_DIR\" \\\n",
    "  --test_data \"$TEST_DATA\" \\\n",
    "  --out_csv \"$OUT\" \\\n",
    "  --batch_size 64"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5122497822515023,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Main",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
