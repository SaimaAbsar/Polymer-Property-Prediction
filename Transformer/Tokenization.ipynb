{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a550767-d264-4a45-9739-f11e5853ec88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "NIPS Kaggle Open Polymer Challenge\n",
    "- This notebook is used to tokenize the SMILES data and extract the embeddings for feature prediction, using the TransPolymer architecture.\n",
    "\n",
    "(Author: Saima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa33f1b3-0870-4c83-82e2-95ef8b7199b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh\n",
    "git clone https://github.com/ChangwenXu98/TransPolymer.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "156d4145-db45-4071-9ebc-6f8289311e64",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "TP_DIR = Path(\"/Transformer/TransPolymer-master\")  # not added to the repo\n",
    "csv_path = Path(\"/data/test.csv\") # Update for train/test data\n",
    "\n",
    "assert TP_DIR.exists(), f\"Path not found: {TP_DIR}\"\n",
    "assert csv_path.exists(), f\"Path not found: {csv_path}\"\n",
    "\n",
    "sys.path.append(str(TP_DIR))\n",
    "\n",
    "VOCAB = TP_DIR / \"tokenizer\" / \"vocab.json\"\n",
    "MERGES = TP_DIR / \"tokenizer\" / \"merges.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efa2d0e0-c6bf-4db3-845b-66d6f58983f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load pretrained-vocab from TransPolymer\n",
    "from PolymerSmilesTokenization import PolymerSmilesTokenizer\n",
    "\n",
    "tok = PolymerSmilesTokenizer(\n",
    "  vocab_file=str(VOCAB),\n",
    "  merges_file=str(MERGES),\n",
    "  bos_token='<s>', eos_token='</s>', sep_token='</s>',\n",
    "  cls_token=\"<s>\", unk_token='<unk>', pad_token='<pad>', mask_token='<mask>',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b69504c6-4eaf-4222-be80-481e5f26e6e7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Test on one molecule\n",
    "s = \"CC(C)COC(=O)C1=CC=CC=C1\"\n",
    "enc = tok(s, padding=\"max_length\", truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "print(\"input_ids\", enc[\"input_ids\"].shape)\n",
    "print(\"attention_mask\", enc[\"attention_mask\"].shape)\n",
    "print(\"first 30 ids: \", enc[\"input_ids\"][0][:30].tolist())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41218d4e-d542-40db-ad6f-a6137dc2f2bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Tokenize SMILES data\n",
    "MAXLEN = 128\n",
    "CHUNK = 32 #1024 for train data\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "smiles = df[\"SMILES\"].astype(str).tolist()\n",
    "\n",
    "ids, masks = [], []\n",
    "for i in range(0, len(smiles), CHUNK):\n",
    "  batch = smiles[i:i+CHUNK]\n",
    "  enc = tok(batch, padding=\"max_length\", truncation=True, max_length=MAXLEN, return_tensors=\"pt\")\n",
    "  ids.append(enc[\"input_ids\"])\n",
    "  masks.append(enc[\"attention_mask\"])\n",
    "\n",
    "  input_ids = torch.cat(ids, dim=0)\n",
    "  attention_mask = torch.cat(masks, dim=0)\n",
    "\n",
    "  out = csv_path.with_suffix(\".tokenized.pt\")\n",
    "  torch.save(\n",
    "    {\"input_ids\": input_ids, \"attention_mask\": attention_mask,\n",
    "     \"row_index\": torch.arange(len(df), dtype=torch.long)}, out\n",
    "  )\n",
    "\n",
    "  print(\"Saved: \", out, \"Shapes: \", input_ids.shape, attention_mask.shape)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5797335263575620,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "Tokenization",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
